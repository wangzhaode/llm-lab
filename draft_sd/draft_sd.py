import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.cache_utils import DynamicCache
import torch.nn.functional as F
import numpy as np
import time
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "1"

# --- å…¨å±€è°ƒè¯•å¼€å…³ ---
DEBUG_MODE = True # è®¾ç½®ä¸º True å¼€å¯è°ƒè¯•ä¿¡æ¯æ‰“å°ï¼Œè®¾ç½®ä¸º False å…³é—­

# --- 1. æ¨¡å‹åŠ è½½ (ä½œä¸ºå…¨å±€å˜é‡ï¼Œåœ¨åº”ç”¨å¯åŠ¨æ—¶åŠ è½½) ---
model_main = None
tokenizer_main = None
model_draft = None
tokenizer_draft = None
main_model_dtype = None
draft_model_dtype = None

# å®šä¹‰æ¨¡å‹è·¯å¾„
# è¯·æ ¹æ®æ‚¨çš„å®é™…æ¨¡å‹è·¯å¾„è¿›è¡Œä¿®æ”¹
MAIN_MODEL_PATH = "/home/yanxing/data/models/Qwen3-4B"
DRAFT_MODEL_PATH = "/home/yanxing/data/models/Qwen3-0.6B"


def print_tensor_summary(tensor, name):
    # å°†å¼ é‡ç§»åŠ¨åˆ°CPUå¹¶è½¬æ¢ä¸ºfloat32ä»¥ä¾¿è®¡ç®—
    tensor_cpu = tensor.detach().to(torch.float32).cpu()
    mean = tensor_cpu.mean().item()
    std = tensor_cpu.std().item()
    abs_sum = tensor_cpu.abs().sum().item()
    # æ‰“å°å‡ ä¸ªæ ·æœ¬å€¼ï¼Œä»å¼ é‡ä¸­é—´å–ä¸€ä¸ª
    sample_val = tensor_cpu.flatten()[tensor_cpu.numel() // 2].item()
    print(f"  {name}:")
    print(f"    - Mean: {mean:.4f}, Std: {std:.4f}, AbsSum: {abs_sum:.4f}, Sample: {sample_val:.4f}")

def print_dynamic_cache_info(
    cache: "DynamicCache",
    cache_name: str = "Cache",
    layer_to_inspect: int = 0,
    title: str = None
):
    if title:
        header = f"--- {title} ---"
    else:
        header = f"--- {cache_name} Info ---"

    print(header)

    if cache is None:
        print("Cache is None.")
        print("-" * (len(header)) + "\n")
        return

    num_layers = len(cache)
    if not (0 <= layer_to_inspect < num_layers):
        print(f"Invalid layer_to_inspect: {layer_to_inspect}. Cache has {num_layers} layers.")
        print("-" * (len(header)) + "\n")
        return

    # --- è·å–è¦æ£€æŸ¥çš„å±‚çš„ Key/Value å¼ é‡ ---
    key, val = cache[layer_to_inspect]
    seq_len = key.shape[2]

    # --- æ‰“å°åŸºæœ¬ä¿¡æ¯ ---
    print(f"Inspecting Layer: {layer_to_inspect}")
    print(f"Sequence Length: {seq_len}")
    print(f"Key Shape:   {key.shape}")
    print(f"Value Shape: {val.shape}")
    print_tensor_summary(key, "Key Cache (Full)")
    print_tensor_summary(val, "Value Cache (Full)")
    print("-" * (len(header)) + "\n")

def format_token_display(token_text):
    """
    å°†tokenæ–‡æœ¬ä¸­çš„ç‰¹æ®Šå­—ç¬¦è½¬æ¢ä¸ºæ›´ç›´è§‚çš„æ˜¾ç¤ºå½¢å¼
    """
    if not token_text:
        return "''"
    # ç‰¹æ®Šå­—ç¬¦æ˜ å°„è¡¨
    special_chars = {
        '\n': '\\n',      # æ¢è¡Œ
        '\r': '\\r',      # å›è½¦
        '\t': '\\t',      # åˆ¶è¡¨ç¬¦
        '\b': '\\b',      # é€€æ ¼
        '\f': '\\f',      # æ¢é¡µ
        '\v': '\\v',      # å‚ç›´åˆ¶è¡¨ç¬¦
        '\a': '\\a',      # å“é“ƒ
        '\0': '\\0',      # ç©ºå­—ç¬¦
        '\\': '\\\\',     # åæ–œæ 
    }
    # æ›¿æ¢ç‰¹æ®Šå­—ç¬¦
    display_text = token_text
    for char, replacement in special_chars.items():
        display_text = display_text.replace(char, replacement)
    # å¤„ç†å…¶ä»–ä¸å¯è§å­—ç¬¦ (ASCII 0-31 å’Œ 127)
    result = ""
    for char in display_text:
        char_code = ord(char)
        if char_code < 32 or char_code == 127:
            # å¯¹äºå…¶ä»–æ§åˆ¶å­—ç¬¦ï¼Œæ˜¾ç¤ºä¸º \xæ ¼å¼
            if char not in special_chars:
                result += f"\\x{char_code:02x}"
            else:
                result += char
        else:
            result += char
    return result

def load_model(model_path):
    try:
        if torch.cuda.is_available() and torch.cuda.is_bf16_supported():
            model_dtype = torch.bfloat16
        else:
            model_dtype = torch.float16
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=model_dtype,
            device_map="auto",
            trust_remote_code=True
        )
        model.eval()
    except Exception as e:
        print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        exit(1)
    return model, tokenizer

class TreeNode:
    """
    è¡¨ç¤ºç”Ÿæˆæ ‘ä¸­çš„ä¸€ä¸ªèŠ‚ç‚¹ã€‚
    Attributes:
        token_id (int): è¯¥èŠ‚ç‚¹ä»£è¡¨çš„ token IDã€‚
        log_prob (float): ä»çˆ¶èŠ‚ç‚¹ç”Ÿæˆæ­¤èŠ‚ç‚¹çš„æ¡ä»¶å¯¹æ•°æ¦‚ç‡ã€‚
        cumulative_log_prob (float): ä»æ ¹åˆ°æ­¤èŠ‚ç‚¹çš„è·¯å¾„çš„ç´¯ç§¯å¯¹æ•°æ¦‚ç‡ã€‚
        parent (TreeNode): çˆ¶èŠ‚ç‚¹å¯¹è±¡ï¼Œæ ¹èŠ‚ç‚¹çš„çˆ¶èŠ‚ç‚¹ä¸º Noneã€‚
        children (list[TreeNode]): å­èŠ‚ç‚¹å¯¹è±¡åˆ—è¡¨ã€‚
        depth (int): èŠ‚ç‚¹åœ¨æ ‘ä¸­çš„æ·±åº¦ (æ ¹ä¸º0)ã€‚
        node_id (int): åœ¨æ ‘ä¸­åˆ›å»ºçš„å”¯ä¸€é¡ºåºIDï¼Œç”¨äºæœ€ç»ˆæ’åºã€‚
    """
    def __init__(self, token_id, log_prob, parent, node_id):
        self.token_id = token_id
        self.log_prob = log_prob
        self.parent = parent
        self.node_id = node_id # å”¯ä¸€çš„ã€æŒ‰åˆ›å»ºé¡ºåºçš„ID
        self.children = []

        if parent:
            self.depth = parent.depth + 1
            self.cumulative_log_prob = parent.cumulative_log_prob + log_prob
            parent.children.append(self)
        else:
            # é€‚ç”¨äºè¿æ¥åˆ°è™šæ‹Ÿæ ¹èŠ‚ç‚¹çš„é¦–å±‚èŠ‚ç‚¹
            self.depth = 0
            self.cumulative_log_prob = log_prob

class Tree:
    """
    ä½¿ç”¨ TreeNode å¯¹è±¡ç®¡ç†è‰ç¨¿ token æ ‘çš„ç”Ÿé•¿ã€å‰ªæå’Œå®šç¨¿ã€‚
    """
    def __init__(self, top_k: int, prompt_len: int, device: torch.device):
        self.top_k = top_k
        self.prompt_len = prompt_len
        self.device = device
        self.logsoftmax = torch.nn.LogSoftmax(dim=-1)
        # build a virtual root node
        self.root = TreeNode(token_id=-1, log_prob=0.0, parent=None, node_id=-1)
        # å½“å‰å¾…æ‰©å±•çš„å¶å­èŠ‚ç‚¹
        self.active_leaves = []
        self._node_counter = 0
        # ç”¨äºé«˜æ•ˆç”Ÿæˆ attention mask çš„å†…éƒ¨çŠ¶æ€ (ä¸ä¸Šä¸€ç‰ˆç›¸åŒ)
        self._inference_mask = torch.eye(self.top_k, device=self.device)[None, None]

    def _get_all_nodes_bfs(self):
        """ä½¿ç”¨å¹¿åº¦ä¼˜å…ˆæœç´¢è¿”å›æ ‘ä¸­æ‰€æœ‰ï¼ˆéè™šæ‹Ÿæ ¹ï¼‰èŠ‚ç‚¹ã€‚"""
        all_nodes = []
        queue = list(self.root.children)
        while queue:
            node = queue.pop(0)
            all_nodes.append(node)
            queue.extend(node.children)
        return all_nodes

    def add_initial_level(self, logits: torch.Tensor):
        log_probs = self.logsoftmax(logits)
        top_k_results = torch.topk(log_probs, self.top_k, dim=-1)
        initial_tokens = top_k_results.indices.squeeze(0)
        initial_scores = top_k_results.values.squeeze(0)

        for i in range(self.top_k):
            node = TreeNode(
                token_id=initial_tokens[i].item(),
                log_prob=initial_scores[i].item(),
                parent=self.root,
                node_id=self._node_counter
            )
            self._node_counter += 1
            self.active_leaves.append(node)

    def grow(self, logits: torch.Tensor):
        log_probs = self.logsoftmax(logits)
        top_k = torch.topk(log_probs, self.top_k, dim=-1)
        topk_tokens, topk_log_probs = top_k.indices, top_k.values
        candidates = []
        # 1. ä¸ºæ¯ä¸ªæ´»è·ƒå¶å­èŠ‚ç‚¹ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„å­èŠ‚ç‚¹
        for i, parent_leaf in enumerate(self.active_leaves):
            for j in range(self.top_k):
                child_node = TreeNode(
                    token_id=topk_tokens[i, j].item(),
                    log_prob=topk_log_probs[i, j].item(),
                    parent=parent_leaf,
                    node_id=self._node_counter
                )
                self._node_counter += 1
                candidates.append(child_node)

        # 2. å‰ªæï¼šæ ¹æ®ç´¯ç§¯æ¦‚ç‡å¯¹æ‰€æœ‰æ–°ç”Ÿæˆçš„å€™é€‰èŠ‚ç‚¹è¿›è¡Œæ’åº
        candidates.sort(key=lambda n: n.cumulative_log_prob, reverse=True)
        # 3. é€‰å‡º top_k ä¸ªæœ€å¥½çš„èŠ‚ç‚¹ä½œä¸ºä¸‹ä¸€è½®çš„æ´»è·ƒå¶å­
        new_active_leaves = candidates[:self.top_k]
        # 4. æ›´æ–° attention mask çš„å†…éƒ¨çŠ¶æ€
        #    æ‰¾å‡ºæ–°å¶å­çš„çˆ¶èŠ‚ç‚¹åœ¨ä¸Šä¸€è½® active_leaves ä¸­çš„ç´¢å¼•
        parent_map = {leaf: i for i, leaf in enumerate(self.active_leaves)}
        parent_remap_indices = [parent_map[node.parent] for node in new_active_leaves]
        parent_remap_indices = torch.tensor(parent_remap_indices, device=self.device)
        self._inference_mask = torch.cat(
            (self._inference_mask[:, :, parent_remap_indices],
             torch.eye(self.top_k, device=self.device)[None, None]),
            dim=3
        )
        # 5. æ›´æ–°æ´»è·ƒå¶å­åˆ—è¡¨
        self.active_leaves = new_active_leaves

    def get_inference_inputs(self):
        """
        è·å–å½“å‰è½®æ¬¡æ¨¡å‹æ¨ç†æ‰€éœ€çš„ input_ids, attention_mask, position_idsã€‚
        """
        if not self.active_leaves:
            raise ValueError("Tree has no active leaves to grow from.")
        # ä»æ´»è·ƒå¶å­èŠ‚ç‚¹å¯¹è±¡ä¸­æå–ä¿¡æ¯ï¼Œç»„è£…æˆå¼ é‡
        input_ids = torch.tensor([[node.token_id for node in self.active_leaves]], device=self.device)
        # æ‰€æœ‰æ´»è·ƒå¶å­éƒ½åœ¨åŒä¸€æ·±åº¦
        current_depth = self.active_leaves[0].depth - 1
        position_ids = torch.full_like(input_ids, self.prompt_len + current_depth, device=self.device)
        # ç”Ÿæˆ attention mask
        left_mask = torch.zeros([1, 1, self.top_k, self.prompt_len], dtype=torch.float32, device=self.device)
        right_mask = (1 - self._inference_mask) * torch.finfo(torch.float32).min
        attention_mask = torch.cat([left_mask, right_mask], dim=-1).to(torch.bfloat16)
        return input_ids, attention_mask, position_ids

    def finalize(self, max_draft_tokens: int):
        """
        ä»æ•´æ£µæ ‘ä¸­é€‰å‡ºæœ€ç»ˆçš„è‰ç¨¿ï¼Œå¹¶ç”Ÿæˆ verifier æ‰€éœ€çš„ mask å’Œ position_idsã€‚
        """
        # 1. è·å–æ‰€æœ‰èŠ‚ç‚¹ï¼ŒæŒ‰åˆ†æ•°æ’åºï¼Œé€‰å‡ºæœ€ç»ˆè‰ç¨¿èŠ‚ç‚¹
        all_nodes = self._get_all_nodes_bfs()
        all_nodes.sort(key=lambda n: n.cumulative_log_prob, reverse=True)
        draft_nodes = all_nodes[:max_draft_tokens]
        # 2. æŒ‰åˆ›å»ºé¡ºåº (node_id) æ’åºï¼Œä»¥ç¡®ä¿çˆ¶èŠ‚ç‚¹æ€»åœ¨å­èŠ‚ç‚¹ä¹‹å‰
        draft_nodes.sort(key=lambda n: n.node_id)
        # 3. ç”Ÿæˆ verifier æ‰€éœ€çš„è¾“å…¥
        draft_tokens = torch.tensor([[node.token_id for node in draft_nodes]], device=self.device)
        tree_position_ids = torch.tensor([node.depth - 1 for node in draft_nodes], device=self.device)
        # 4. æ„å»º tree_mask
        node_id_to_idx = {node.node_id: i for i, node in enumerate(draft_nodes)}
        tree_mask = torch.eye(max_draft_tokens, device=self.device, dtype=torch.bool)
        for i, node in enumerate(draft_nodes):
            if node.parent and node.parent.node_id in node_id_to_idx:
                parent_idx = node_id_to_idx[node.parent.node_id]
                # æ¯ä¸ªèŠ‚ç‚¹çš„ attention mask æ˜¯å…¶çˆ¶èŠ‚ç‚¹çš„ mask åŠ ä¸Šå®ƒè‡ªå·±
                tree_mask[i].add_(tree_mask[parent_idx])
        tree_mask = tree_mask.float()[None, None]

        # 5. æ„å»º retrieve_indices
        leaf_nodes = [node for node in draft_nodes if not any(child in draft_nodes for child in node.children)]
        max_depth = 0
        if draft_nodes:
            max_depth = max(node.depth for node in draft_nodes)
        paths = []
        for leaf in leaf_nodes:
            path = []
            curr = leaf
            while curr.parent:
                path.append(node_id_to_idx[curr.node_id])
                curr = curr.parent
            paths.append(list(reversed(path)))
        retrieve_indices = torch.full((len(paths), max_depth), -1, dtype=torch.long, device=self.device)
        for i, path in enumerate(paths):
            retrieve_indices[i, :len(path)] = torch.tensor(path, dtype=torch.long, device=self.device)
        return draft_tokens, tree_mask, tree_position_ids, retrieve_indices

    def __repr__(self):
        if not self.root.children:
            return "<Tree (empty)>"
        lines = ["Tree Structure (token, cumulative_log_prob):"]
        lines.append(self._build_repr_recursive(self.root, ""))
        return "\n".join(lines)

    def _build_repr_recursive(self, node, prefix):
        lines = []
        children = node.children
        for i, child in enumerate(children):
            is_last = (i == len(children) - 1)
            connector = "â””â”€â”€ " if is_last else "â”œâ”€â”€ "
            token_str = format_token_display(self.tokenizer.decode([child.token_id]))
            node_info = f"'{token_str}' ({child.token_id}, {child.cumulative_log_prob:.2f})"
            lines.append(f"{prefix}{connector}{node_info}")
            new_prefix = prefix + ("    " if is_last else "â”‚   ")
            child_repr = self._build_repr_recursive(child, new_prefix)
            if child_repr:
                lines.append(child_repr)
        return "\n".join(lines)

class DraftModel:
    def __init__(self, model_path):
        self.model, self.tokenizer = load_model(model_path)
        self.logsoftmax = torch.nn.LogSoftmax(dim=-1)
        self.depth = 8
        self.top_k = 5
        self.max_draft_tokens = self.depth * self.top_k
        self.past_key_values = None
        self.past_len = 0

    @torch.no_grad()
    def update_kv_cache(self):
        if self.past_key_values is None:
            return
        for layer_idx in range(len(self.past_key_values)):
            self.past_key_values.key_cache[layer_idx] = self.past_key_values.key_cache[layer_idx][:, :, :self.past_len, :]
            self.past_key_values.value_cache[layer_idx] = self.past_key_values.value_cache[layer_idx][:, :, :self.past_len, :]
        self.past_key_values._seen_tokens = self.past_len

    @torch.no_grad()
    def generate(self, input_ids):
        self.update_kv_cache()
        input_ids = input_ids.to(self.model.device)
        self.past_len += input_ids.shape[1]
        tree = Tree(top_k=self.top_k, prompt_len=self.past_len, device=self.model.device)
        with torch.no_grad():
            outputs = self.model(input_ids, past_key_values=self.past_key_values, use_cache=True)
            logits = outputs.logits[:, -1, :]
            self.past_key_values = outputs.past_key_values

        tree.add_initial_level(logits)
        for _ in range(self.depth):
            input_ids, attention_mask, position_ids = tree.get_inference_inputs()
            with torch.no_grad():
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    position_ids=position_ids,
                    past_key_values=self.past_key_values,
                    use_cache=True
                )
            self.past_key_values = outputs.past_key_values
            tree.grow(outputs.logits[0])
        draft_tokens, tree_mask, tree_position_ids, retrieve_indices = tree.finalize(self.max_draft_tokens)
        if DEBUG_MODE and False:
            tree.tokenizer = self.tokenizer
            print(tree)
            print("\n--- Final Draft ---")
            print(f"Draft Tokens: {draft_tokens}")
            print(f"Tree Mask Shape: {tree_mask.shape}")
            print(f"Tree Position IDs: {tree_position_ids}")
            print(f"Retrieve Indices:\n{retrieve_indices}")
        return draft_tokens, tree_mask, tree_position_ids, retrieve_indices

class MainModel:
    def __init__(self, model_path):
        self.model, self.tokenizer = load_model(model_path)
        self.past_len = 0
        self.past_key_values = None
        self.tokens = []
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    def gen_position_ids(self, prompt_len, draft_position_ids):
        position_ids = torch.arange(prompt_len, device=self.device)
        position_ids = torch.concat([position_ids, draft_position_ids + prompt_len], dim=0)
        position_ids = position_ids.unsqueeze(0) + self.past_len
        return position_ids

    def gen_attention_mask(self, prompt_len, draft_attention_mask):
        '''
        [1, 0, 0, 0, 0]  [0, 0, 0]
        [1, 1, 0, 0, 0]  [0, 0, 0]
        [1, 1, 1, 0, 0]  [0, 0, 0]
        [1, 1, 1, 1, 0]  [0, 0, 0]
        [1, 1, 1, 1, 1]  [0, 0, 0]

        [1, 1, 1, 1, 1]  [ draft ]
        [1, 1, 1, 1, 1]  [  pos  ]
        [1, 1, 1, 1, 1]  [  ids  ]
        '''
        draft_len = draft_attention_mask.shape[-1]
        up_left_mask = torch.tril(torch.ones([1, 1, prompt_len, prompt_len], device=self.device))
        up_right_mask = torch.zeros([1, 1, prompt_len, draft_len], device=self.device)
        up_mask = torch.concat([up_left_mask, up_right_mask], dim=-1)
        down_left = torch.ones([1, 1, draft_len, prompt_len], device=self.device)
        down_right = draft_attention_mask.to(self.device)
        down_mask = torch.concat([down_left, down_right], dim=-1)
        attention_mask = torch.concat([up_mask, down_mask], dim=-2)
        if self.past_len > 0:
            past_mask = torch.ones([1, 1, prompt_len + draft_len, self.past_len], device=self.device)
            attention_mask = torch.concat([past_mask, attention_mask], dim=-1)
        attention_mask = (1 - attention_mask) * torch.finfo(torch.float32).min
        return attention_mask.to(torch.bfloat16)

    @torch.no_grad()
    def update_kv_cache(self,
        accepted_indices,
        prompt_len,
        accept_length,
    ):
        for layer_idx in range(len(self.past_key_values)):
            key_cache, value_cache = self.past_key_values[layer_idx]
            device = key_cache.device
            destination_slice_key = key_cache[:, :, prompt_len : prompt_len + accept_length, :]
            destination_slice_value = value_cache[:, :, prompt_len : prompt_len + accept_length, :]
            accepted_keys = key_cache.index_select(dim=2, index=accepted_indices)
            accepted_values = value_cache.index_select(dim=2, index=accepted_indices)
            destination_slice_key.copy_(accepted_keys)
            destination_slice_value.copy_(accepted_values)
        new_seq_len = prompt_len + accept_length
        for layer_idx in range(len(self.past_key_values)):
            self.past_key_values.key_cache[layer_idx] = self.past_key_values.key_cache[layer_idx][:, :, :new_seq_len, :]
            self.past_key_values.value_cache[layer_idx] = self.past_key_values.value_cache[layer_idx][:, :, :new_seq_len, :]
        self.past_key_values._seen_tokens = new_seq_len
        self.past_len = new_seq_len.item()

    def evaluate_posterior(self,
            logits: torch.Tensor,
            candidates: torch.Tensor,
            ):
        # Find the tokens that match the maximum logits for each position in the sequence
        verify_tokens = torch.argmax(logits, dim=-1)
        posterior_mask = (candidates.to(logits.device) == verify_tokens[:, :-1]).int()
        candidates_accept_length = (torch.cumprod(posterior_mask, dim=1)).sum(dim=1)
        accept_length = candidates_accept_length.max()
        # Choose the best candidate
        if accept_length == 0:
            # Default to the first candidate if none are accepted
            best_candidate = torch.tensor(0, dtype=torch.long, device=candidates.device)
        else:
            best_candidate = torch.argmax(candidates_accept_length).to(torch.long)
        accept_ids = verify_tokens[None, best_candidate, :accept_length + 1]
        return best_candidate, accept_length, accept_ids, logits[best_candidate, accept_length]

    @torch.no_grad()
    def generate(self, input_ids):
        seq_len = input_ids.shape[1]
        debug = False
        if self.past_len == 0:
            self.past_len = seq_len
            attention_mask = torch.tril(torch.ones([1, 1, seq_len, seq_len], device=self.device))
            attention_mask = (1 - attention_mask) * torch.finfo(torch.float32).min
            position_ids = torch.arange(seq_len, device=self.device).unsqueeze(0)
        else:
            attention_mask = torch.zeros([1, 1, 1, self.past_len + 1], dtype=torch.long, device=self.device)
            position_ids = torch.arange(seq_len, device=self.device).unsqueeze(0) + self.past_len
            self.past_len += 1
            # if input_ids.item() == 68805: debug = True
        if debug:
            print('\nself.past_len = ', self.past_len)
            print('position_ids = ', position_ids)
            print('attention_mask.shape = ', attention_mask.shape)
            print_dynamic_cache_info(self.past_key_values)
        outputs = self.model(
            input_ids,
            attention_mask = attention_mask.to(torch.bfloat16),
            position_ids=position_ids,
            past_key_values=self.past_key_values
        )
        logits = outputs.logits[0, -1, :]
        if debug:
            # print('\n', self.tokens)
            # print(self.tokenizer.batch_decode(torch.tensor(self.tokens)))
            print(logits)
            print(torch.topk(logits, k=5))
            exit(0)
        self.past_key_values = outputs.past_key_values
        token = torch.argmax(logits, dim=-1).reshape([1, 1])
        self.tokens.append(token.item())
        return token

    @torch.no_grad()
    def verify(self,
            draft_tokens,
            draft_position_ids,
            draft_attention_mask,
            retrieve_indices,
            prompt_ids
            ):
        prompt_len = prompt_ids.shape[1]
        input_ids = torch.concat([prompt_ids, draft_tokens], dim=1)
        position_ids = self.gen_position_ids(prompt_len, draft_position_ids)
        attention_mask = self.gen_attention_mask(prompt_len, draft_attention_mask)
        outputs = self.model(
            input_ids,
            attention_mask = attention_mask,
            position_ids=position_ids,
            past_key_values=self.past_key_values
        )
        self.past_key_values = outputs.past_key_values
        logits = outputs.logits
        # draft candidates
        padding = (torch.zeros(1, 1, dtype=torch.long) - 1).to(self.device)
        draft_tokens = torch.cat((draft_tokens, padding), dim=1)
        candidates = draft_tokens[0, retrieve_indices]
        # draft logits
        logits = logits[0, prompt_len - 1:, :]
        logits_retrieve_indices = F.pad(retrieve_indices + 1, pad=(1, 0, 0, 0), mode='constant', value=0)
        logits = logits[logits_retrieve_indices]
        # print(f'logits = {torch.argmax(logits, dim=-1)}, txt = {self.tokenizer.batch_decode(torch.argmax(logits, dim=-1))}')
        candidates_valid = candidates.clone()
        candidates_valid[candidates < 0] = 0
        # print(f'candidates = {candidates}, txt = {self.tokenizer.batch_decode(candidates_valid)}')
        best_candidate, accept_length, accept_ids, sample_p = self.evaluate_posterior(logits, candidates)
        accepted_indices = retrieve_indices[best_candidate, :accept_length] + self.past_len + prompt_len
        self.update_kv_cache(accepted_indices, self.past_len + prompt_len, accept_length)
        return best_candidate, accept_length, accept_ids, sample_p

class SpeculativeModel:
    def __init__(self, main_path, draft_path):
        self.main_model = MainModel(main_path)
        self.draft_model = DraftModel(draft_path)
        self.tokenizer = self.main_model.tokenizer
        self.device = self.main_model.model.device
        self.stop_id = self.tokenizer.eos_token_id

    @torch.no_grad()
    def generate(self,
                prompt,
                use_speculative=True,
                max_new_tokens=200):
        messages = [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": prompt}]
        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        input_ids = self.tokenizer.encode(text, add_special_tokens=True)
        input_ids = torch.tensor([input_ids], device=self.device)
        prompt_ids = input_ids
        cur_new_tokens = 0
        while cur_new_tokens < max_new_tokens:
            if not use_speculative:
                input_ids = self.main_model.generate(input_ids)
                cur_new_tokens += 1
                yield input_ids
                continue
            draft_tokens, tree_mask, tree_position_ids, retrieve_indices = self.draft_model.generate(input_ids)
            best_candidate, accept_length, accept_ids, sample_p = self.main_model.verify(
                draft_tokens,
                draft_position_ids=tree_position_ids,
                draft_attention_mask=tree_mask,
                retrieve_indices=retrieve_indices,
                prompt_ids=prompt_ids,
            )
            input_ids = accept_ids
            cur_new_tokens += input_ids.shape[1]
            # print(self.tokenizer.batch_decode(input_ids))
            yield input_ids
            prompt_ids = accept_ids[:, -1:]
            if self.stop_id in input_ids.tolist():
                break
        # print(f"Average Accept Length: {avg_accept / max_new_tokens}")


def webui(spec_model):
    def webui_generate(
        prompt,
        highlight_accepted_tokens, # Keep this for UI, even if always True in simple mode
        max_new_tokens,
        max_draft_tokens,
        num_branches,
        max_depth,
        # Removed: temperature, top_p, top_k, use_speculative_decoding
    ):
        """Gradioçš„æ§åˆ¶å™¨å‡½æ•°ï¼Œå¤„ç†ç”¨æˆ·è¾“å…¥å¹¶è°ƒç”¨æ¨¡å‹ç”Ÿæˆã€‚"""
        if spec_model is None:
            yield "<h3><font color='red'>é”™è¯¯ï¼šæ¨¡å‹å°šæœªåŠ è½½ï¼Œè¯·æ£€æŸ¥æ§åˆ¶å°æ—¥å¿—ã€‚</font></h3>", "0.00 tokens/s", "0.00"
            return

        full_text_html = ""
        total_token_count = 0
        total_llm_calls = 0
        start_time = time.time()

        try:
            generator = spec_model.generate(
                prompt=prompt,
                use_speculative = False,
                max_new_tokens=max_new_tokens,
                # max_draft_tokens=max_draft_tokens,
                # num_branches=num_branches,
                # max_depth=max_depth,
                # # Removed: temperature, top_p, top_k, use_speculative_decoding (always True implied)
            )

            for output_ids in generator:
                output_ids = output_ids.reshape(-1, 1)
                token_count = output_ids.shape[0]
                decode_token = output_ids[:1, 0]
                decode_txt = spec_model.tokenizer.batch_decode(decode_token)[0]
                full_text_html += f"{decode_txt}"
                if token_count > 1:
                    accepted_tokens = output_ids[1:, 0]
                    accepted_txt = ''.join(spec_model.tokenizer.batch_decode(accepted_tokens))
                    full_text_html += f"<span style='color: orange; font-weight: bold;'>{accepted_txt}</span>"

                total_token_count += token_count
                total_llm_calls += 1
                elapsed_time = time.time() - start_time
                speed = total_token_count / elapsed_time if elapsed_time > 0 else 0
                compression_ratio = total_token_count / total_llm_calls if total_llm_calls > 0 else 0

                yield full_text_html, f"{speed:.2f} tokens/s", f"{compression_ratio:.2f}"

        except Exception as e:
            print(f"ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            full_text_html += f"<br><h3><font color='red'>ç”Ÿæˆä¸­æ–­: {e}</font></h3>"
            yield full_text_html, "Error", "Error"
    import gradio as gr
    with gr.Blocks(theme=gr.themes.Soft(), css=".gradio-container { max-width: 1200px; margin: auto; }") as demo:
        gr.Markdown(
            """
            # ğŸŒ³ æ ‘å½¢æŠ•æœºè§£ç  WebUI
            ä½¿ç”¨ Qwen3-4B ä½œä¸ºä¸»æ¨¡å‹ï¼ŒQwen3-0.6B ä½œä¸ºè‰ç¨¿æ¨¡å‹è¿›è¡Œæ ‘å½¢æŠ•æœºè§£ç ã€‚
            - **Speed (tokens/s)**: æ¯ç§’ç”Ÿæˆçš„tokenæ•°é‡ã€‚
            - **Compression Ratio**: ç”Ÿæˆçš„tokenæ€»æ•° / ä¸»æ¨¡å‹(LLM)å‰å‘æ¨ç†æ¬¡æ•°ã€‚è¯¥å€¼å¤§äº1è¡¨ç¤ºæœ‰åŠ é€Ÿæ•ˆæœã€‚
            """
        )

        with gr.Row():
            speed_display = gr.Textbox(label="Speed", value="0.00 tokens/s", interactive=False, scale=1)
            compression_ratio_display = gr.Textbox(label="Compression Ratio", value="0.00", interactive=False, scale=1)

        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### âš™ï¸ è®¾ç½®")
                # use_speculative_decoding_checkbox = gr.Checkbox(label="âœ… å¯ç”¨æŠ•æœºè§£ç ", value=True, visible=False) # Removed for simplicity
                highlight_accepted_tokens_checkbox = gr.Checkbox(
                    label="ğŸ¨ é«˜äº®æŠ•æœºéƒ¨åˆ† (æ©™è‰²)", value=True
                )
                prompt_input = gr.Textbox(lines=3, label="ğŸ“ è¾“å…¥æç¤ºè¯", value="è¯·ç”¨ä¸­æ–‡å†™ä¸€é¦–å…³äºäººå·¥æ™ºèƒ½æœªæ¥å‘å±•çš„äº”è¨€ç»å¥è¯—ã€‚")

                with gr.Accordion("æ¨¡å‹å‚æ•°", open=True):
                    max_new_tokens_slider = gr.Slider(minimum=10, maximum=1024, value=256, step=1, label="æœ€å¤§æ–°Tokenæ•°")
                    # Removed: temperature_slider, top_p_slider, top_k_slider

                with gr.Accordion("æŠ•æœºè§£ç å‚æ•°", open=True):
                    max_depth_slider = gr.Slider(minimum=1, maximum=10, value=5, step=1, label="æ ‘æœ€å¤§æ·±åº¦")
                    num_branches_slider = gr.Slider(minimum=1, maximum=10, value=3, step=1, label="æ¯èŠ‚ç‚¹åˆ†æ”¯æ•° (k)")
                    max_draft_tokens_slider = gr.Slider(minimum=1, maximum=50, value=15, step=1, label="è‰ç¨¿æœ€å¤§Tokenæ•°")

                with gr.Row():
                    send_btn = gr.Button("ğŸš€ ç”Ÿæˆ", variant="primary")
                    stop_btn = gr.Button("â¹ï¸ åœæ­¢")
                    clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©º")

            with gr.Column(scale=2):
                gr.Markdown("### ğŸ’¡ è¾“å‡ºç»“æœ")
                output_text = gr.HTML(label="Output", value="")

        send_event = send_btn.click(
            fn=webui_generate,
            inputs=[
                prompt_input, highlight_accepted_tokens_checkbox, # Removed use_speculative_decoding_checkbox
                max_new_tokens_slider, max_draft_tokens_slider, num_branches_slider, max_depth_slider,
                # Removed: temperature_slider, top_p_slider, top_k_slider
            ],
            outputs=[output_text, speed_display, compression_ratio_display],
        )

        stop_btn.click(fn=None, inputs=None, outputs=None, cancels=[send_event], queue=False)

        def clear_action():
            return "", "", "0.00 tokens/s", "0.00"

        clear_btn.click(
            fn=clear_action, inputs=[],
            outputs=[prompt_input, output_text, speed_display, compression_ratio_display],
            cancels=[send_event], queue=False
        )
        demo.launch(share=False, enable_queue=True, server_name="0.0.0.0")

def simple(spec_model):
    RESET = "\033[0m"
    GREEN = "\033[32;1m"
    YELLOW = "\033[33;4m"
    prompt = "è¯·ç”¨ä¸­æ–‡å†™ä¸€é¦–å…³äºäººå·¥æ™ºèƒ½æœªæ¥å‘å±•çš„äº”è¨€ç»å¥è¯—ã€‚"
    generator = spec_model.generate(prompt, use_speculative = False, max_new_tokens = 500)
    all_tokens = 0
    decode_times = 0
    start_time = time.time()
    for i, output_ids in enumerate(generator):
        output_ids = output_ids.reshape(-1, 1)
        all_tokens += output_ids.shape[0]
        decode_times += 1
        decode_token = output_ids[:1, 0]
        accepted_tokens = output_ids[1:, 0]
        decode_txt = spec_model.tokenizer.batch_decode(decode_token)[0]
        accepted_txt = ''.join(spec_model.tokenizer.batch_decode(accepted_tokens))
        # txts = spec_model.tokenizer.batch_decode(output_ids)
        # print(''.join(txts), end="", flush=True)
        print(f"{decode_txt}{YELLOW}{accepted_txt}{RESET}", end="", flush=True)
    elapsed_time = time.time() - start_time
    print(f"\nSpeed: {all_tokens / elapsed_time} tokens/s")
    print(f"\nAverage Accept Length: {all_tokens / decode_times}")

if __name__ == "__main__":
    spec_model = SpeculativeModel(MAIN_MODEL_PATH, DRAFT_MODEL_PATH)
    simple(spec_model)
    # webui(spec_model)
